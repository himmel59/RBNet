import math
import os
import re
import shutil
import time
import numpy as np
import torch
from dataset import DatasetNewNet
import lossfunc as lf

from torch.utils.data import DataLoader
from torch.utils.tensorboard.writer import SummaryWriter
from network import MLP2
# from logger import Logger
from typing import List, Union

RATIO = {'hartree': 1, 'kcal/mol': 627.51, 'kj/mol': 2625.5}
LEARNING_RATE_START = 1e-12
LEARNING_RATE_MAX = 1e-6
LEARNING_RATE_END = 1e-12
BATCH_SIZE = 8
EPOCHS = 2000
# ETA is a small amount for log to avoid negative infinity
ETA = 1e-12
UNIT = 'kj/mol'

def detection_collate(batch: list):
    # in a batch, there're N reactions, M species in each reactions, L points in each species
    torch.set_default_dtype(torch.float64)

    # what we need:
    # features: 16 * LMN
    # factors: 1 * LMN
    # masks: N * LMN
    # labels: 2 * LMN
    # reactions gt: 1 * N
    # reactions original: 1 * N

    features = []  # (LMN, 16)
    factors = []  # (LMN, 1)
    labels = []  # (LMN, 2)
    masks = []  # (LMN, N)
    gt_energies = []  # (N, 1)
    original_energies = []  # (N, 1)

    for reaction in batch:
        # reaction is a dict containing 'gt' and other species which has suffix of dataset and its count

        # actual energy of reactionï¼Œ
        e = 0.  # UNIT: Ha
        mask = []
        for species in reaction:
            if species == 'gt':
                gt_energies.append(reaction['gt'])
            else:
                item = DATASET[species]
                features.append(item.getinputdata())
                factors.append(item.getweight() * reaction[species])
                
                # code below makes label generated by intersection of current status and final status
                # try:
                #     final = re.sub('_[0-9]*@', '_f@', species, 1)
                #     labels.append(DATASET[final].getlabel() & item.getlabel())
                # except:
                #     labels.append(item.getlabel())

                # code below makes label generated by final status
                try:
                    final = re.sub('_[0-9]*@', '_f@', species, 1)
                    labels.append(DATASET[final].getlabel())
                except:
                    labels.append(item.getlabel())

                # code below makes label generated by current status
                # labels.append(item.getlabel())
               
                mask.append(torch.ones_like(item.getweight()))
                e += reaction[species] * item.summary['e_tot']  # UNIT: Ha
        e *= RATIO['kcal/mol'] / RATIO['hartree']  # UNIT: Ha -> kcal/mol
        masks.append(torch.cat(mask, dim=1))
        original_energies.append(e)

    features = torch.cat(features, dim=1).T
    factors = torch.cat(factors, dim=1).T
    masks = torch.block_diag(*masks).T
    labels = torch.cat(labels, dim=0)
    gt_energies = torch.tensor(gt_energies).unsqueeze(1)
    original_energies = torch.tensor(original_energies).unsqueeze(1)
    return features, factors, masks, labels, gt_energies, original_energies


def load_net(checkpoint: str, device):
    start_epoch = 0
    if os.path.exists(checkpoint):
        if os.path.splitext(checkpoint)[-1] == '.pth':
            checkpoint = torch.load(checkpoint)
            exc_mnet = MLP2()
            if torch.cuda.device_count() > 1:
                device_ids = list(range(torch.cuda.device_count()))
                exc_mnet = torch.nn.DataParallel(exc_mnet,
                                                 device_ids=device_ids)
            exc_mnet.to(device=device)
            exc_mnet.load_state_dict(checkpoint['net'])  # type: ignore
            optimizer = torch.optim.Adam(exc_mnet.parameters())
            optimizer.load_state_dict(checkpoint['optimizer'])  # type: ignore
            start_epoch = int(checkpoint['epoch'])  # type: ignore
        elif os.path.splitext(checkpoint)[-1] == '.net':
            if device.type == 'cpu':
                exc_mnet = torch.load(
                    checkpoint, map_location=torch.device('cpu')).module.cpu()
            else:
                exc_mnet = torch.load(checkpoint).to(device)
                if torch.cuda.device_count() > 1:
                    device_ids = list(range(torch.cuda.device_count()))
                    exc_mnet.device_ids = device_ids
            if isinstance(exc_mnet, torch.nn.DataParallel):
                optimizer = torch.optim.Adam(exc_mnet.module.parameters())
            else:
                optimizer = torch.optim.Adam(exc_mnet.paramenters())
        else:
            print("loading error")
            raise Exception("file extension error")

    else:
        exc_mnet = MLP2()
        optimizer = torch.optim.Adam(exc_mnet.parameters())
        if torch.cuda.device_count() > 1:
            device_ids = list(range(torch.cuda.device_count()))
            exc_mnet = torch.nn.DataParallel(exc_mnet, device_ids=device_ids)
        exc_mnet.to(device)
    return exc_mnet, optimizer, start_epoch

def save_net(work_dir: str, name: str, exc_ment: MLP2, checkpoint: str) -> None:
    save_path = os.path.join(work_dir, "checkpoint")
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    torch.save(checkpoint, os.path.join(save_path, f"ckpt_{name}.pth"))
    torch.save(exc_ment, os.path.join(save_path, f"ntwk_{name}.net"))

def printout(epoch: int,
             timecost: float,
             loss: torch.Tensor,
             lr: float,
             training: torch.Tensor,
             validate: Union[torch.Tensor, None] = None,
             points: Union[List[torch.Tensor], None] = None):
    np.set_printoptions(suppress=True)
    mode = 0
    loss = loss.cpu().numpy()
    training = training.cpu().numpy()
    if validate is not None:
        mode += 1
        validate = validate.cpu().numpy()
    if points is not None:
        mode += 2
        points = [item.type(torch.int).cpu().numpy() for item in points]

    # mode 0: validate x; points x;
    # mode 1: validate v; points x;
    # mode 2: validate x; points v;
    # mode 3: validate v; points v;

    # public part, info
    if loss.ndim == 1:
        loss = loss[0]
    elif loss.ndim == 2:
        loss = loss[0][0]
    else:
        raise Exception

    print("", end='\n')
    print(f"epoch\t{'{0: <7}'.format(epoch)[:7]}\t", end='')
    print(f"time\t{'{0: <39}'.format(timecost)[:39]}\t", end='\n')
    print(f"loss\t{'{0: <23}'.format(loss)[:23]}\t", end='')
    print(f"lr\t{'{0: <23}'.format(lr)[:23]}\t", end='\n')

    # training diff part
    print("training\tUNIT: kcal/mol", end='\n')
    exc, prev, post, ought, real = training.T
    diff1_list = prev - exc  # error before action
    diff2_list = post - exc  # error after action
    diff3_list = ought      # change in plan
    diff4_list = real       # change after action
    diff1_list = [eval(str(item)) for item in list(diff1_list)]
    diff2_list = [eval(str(item)) for item in list(diff2_list)]
    diff3_list = [eval(str(item)) for item in list(diff3_list)]
    diff4_list = [eval(str(item)) for item in list(diff4_list)]
    for i in range(3 - (len(exc) - 1) % 4):
        diff1_list.append("---------------")
        diff2_list.append("---------------")
        diff3_list.append("---------------")
        diff4_list.append("---------------")
    for i in range(0, len(diff1_list) // 4):
        for j in range(4):
            print((
                f"\033[0;37;40m{(str(diff1_list[i * 4 + j]) + '0' * 15)[:15]}\033[0m"
            ),
                end='\n' if j == 3 else '\t')
        for j in range(4):
            print((
                f"\033[0;37;40m{(str(diff2_list[i * 4 + j]) + '0' * 15)[:15]}\033[0m"
            ),
                end='\n' if j == 3 else '\t')
        for j in range(4):
            print((
                f"\033[0;36;40m{(str(diff3_list[i * 4 + j]) + '0' * 15)[:15]}\033[0m"
            ),
                end='\n' if j == 3 else '\t')
        for j in range(4):
            print((
                f"\033[0;36;40m{(str(diff4_list[i * 4 + j]) + '0' * 15)[:15]}\033[0m"
            ),
                end='\n' if j == 3 else '\t')

    # training point part
    if mode >= 2:
        assert points is not None
        training_points = points[0]
        training_points = training_points.T
        """
        right_drop means should drop and actually drop
        error_drop means should keep and actually drop
        right_keep means should drop and actually keep
        error_keep means should keep and actually keep
        """
        training_right_drop = training_points[0]
        training_error_drop = training_points[1]
        training_right_keep = training_points[2]
        training_error_keep = training_points[3]
        training_drop_point = training_right_drop + training_error_drop
        training_all_points = training_right_drop + training_error_drop + training_right_keep + training_error_keep

        point11_list = [
            f"{item[0]}/{item[1]}"
            for item in list(zip(training_drop_point, training_all_points))
        ]
        point12_list = [
            f"{'{0:<6}'.format(item[0])} {'{0:<6}'.format(item[1])}"
            for item in list(zip(training_right_keep, training_error_keep))
        ]
        point13_list = [
            f"{'{0:<6}'.format(item[0])} {'{0:<6}'.format(item[1])}"
            for item in list(zip(training_right_drop, training_error_drop))
        ]
        for i in range(3 - (len(point11_list) - 1) % 4):
            point11_list.append("---------------")
            point12_list.append("---------------")
            point13_list.append("---------------")
        for i in range(0, len(point11_list) // 4):
            for j in range(4):
                print((
                    f"\033[0;35;40m{(str(point11_list[i * 4 + j]) + ' ' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')
            for j in range(4):
                print((
                    f"\033[0;35;40m{(str(point12_list[i * 4 + j]) + ' ' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')
            for j in range(4):
                print((
                    f"\033[0;35;40m{(str(point13_list[i * 4 + j]) + ' ' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')

    # validate part
    if mode % 2 != 0:
        assert validate is not None
        print("validate", end='\n')
        exc, prev, post, ought, real = validate.T
        diff1_list = prev - exc
        diff2_list = post - exc
        diff3_list = ought
        diff4_list = real
        diff1_list = [eval(str(item)) for item in list(diff1_list)]
        diff2_list = [eval(str(item)) for item in list(diff2_list)]
        diff3_list = [eval(str(item)) for item in list(diff3_list)]
        diff4_list = [eval(str(item)) for item in list(diff4_list)]
        for i in range(3 - (len(exc) - 1) % 4):
            diff1_list.append("---------------")
            diff2_list.append("---------------")
            diff3_list.append("---------------")
            diff4_list.append("---------------")
        for i in range(0, len(diff1_list) // 4):
            for j in range(4):
                print((
                    f"\033[0;37;40m{(str(diff1_list[i * 4 + j]) + '0' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')
            for j in range(4):
                print((
                    f"\033[0;37;40m{(str(diff2_list[i * 4 + j]) + '0' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')
            for j in range(4):
                print((
                    f"\033[0;36;40m{(str(diff3_list[i * 4 + j]) + '0' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')
            for j in range(4):
                print((
                    f"\033[0;36;40m{(str(diff4_list[i * 4 + j]) + '0' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')

    # validate points
    if mode == 3:
        assert points is not None and len(points) == 2
        validate_points = points[1]
        if validate_points is None:
            raise
        validate_points = points[1]
        validate_points = validate_points.T
        """
        right_drop means should drop and actually drop
        error_drop means should keep and actually drop
        right_keep means should drop and actually keep
        error_keep means should keep and actually keep
        """
        validate_right_drop = validate_points[0]
        validate_error_drop = validate_points[1]
        validate_right_keep = validate_points[2]
        validate_error_keep = validate_points[3]
        validate_drop_point = validate_right_drop + validate_error_drop
        validate_all_points = validate_right_drop + validate_error_drop + validate_right_keep + validate_error_keep

        point21_list = [
            f"{item[0]}/{item[1]}"
            for item in list(zip(validate_drop_point, validate_all_points))
        ]
        point22_list = [
            f"{'{0:<6}'.format(item[0])} {'{0:<6}'.format(item[1])}"
            for item in list(zip(validate_right_keep, validate_error_keep))
        ]
        point23_list = [
            f"{'{0:<6}'.format(item[0])} {'{0:<6}'.format(item[1])}"
            for item in list(zip(validate_right_drop, validate_error_drop))
        ]
        for i in range(3 - (len(point21_list) - 1) % 4):
            point21_list.append("---------------")
            point22_list.append("---------------")
            point23_list.append("---------------")
        for i in range(0, len(point21_list) // 4):
            for j in range(4):
                print((
                    f"\033[0;35;40m{(str(point21_list[i * 4 + j]) + ' ' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')
            for j in range(4):
                print((
                    f"\033[0;35;40m{(str(point22_list[i * 4 + j]) + ' ' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')
            for j in range(4):
                print((
                    f"\033[0;35;40m{(str(point23_list[i * 4 + j]) + ' ' * 15)[:15]}\033[0m"
                ),
                    end='\n' if j == 3 else '\t')


def writeout(writer_number: int,
             loss_tuple: torch.Tensor,
             lr: float,
             training: torch.Tensor,
             validate: Union[torch.Tensor, None] = None,
             points: Union[List[torch.Tensor], None] = None):
    np.set_printoptions(suppress=True)
    loss_tuple = loss_tuple.cpu().numpy()
    training = training.cpu().numpy()

    mode = 0
    if validate is not None:
        mode += 1
        validate = validate.cpu().numpy()
    if points is not None:
        mode += 2
        points = [item.cpu().numpy() for item in points]

    # public part
    loss_training, loss_training_detail = loss_tuple[0][0], loss_tuple[0][1:]
    loss = {"loss_training": loss_training}
    for i in range(len(loss_training_detail)):
        loss.update({f"loss_training{i+1}": loss_training_detail[i]})

    if len(loss_tuple) == 2:
        loss_validate, loss_validate_detail = loss_tuple[1][0], loss_tuple[1][1:]
        loss.update({f"loss_validate": loss_validate})
        for i in range(len(loss_validate_detail)):
            loss.update({f"loss_validate{i+1}": loss_validate_detail[i]})

    writer.add_scalars("loss", loss, writer_number)
    writer.add_scalar("lr", lr, writer_number)
    '''
    in difference lists:
    1 difference before execution 
    2 difference after execution 
    3 change in plan
    4 change in execution
    '''

    # training diff
    exc, prev, post, ought, real = training.T
    diff11_list = prev - exc
    diff12_list = post - exc
    diff13_list = ought
    diff14_list = real

    writer.add_scalars(
        "training_item",
        dict(
            zip(list(map(str, list(range(1,
                                         len(diff11_list) + 1)))),
                diff11_list)), writer_number)

    if mode % 2 == 0:
        writer.add_scalars(
            "difference", {
                'training_prev': np.mean(np.abs(diff11_list)),
                'training_post': np.mean(np.abs(diff12_list)),
                'training_plan': np.mean(np.abs(diff13_list)),
                'training_exec': np.mean(np.abs(diff14_list)),
            }, writer_number)
        writer.add_scalars(
            "variance", {
                'training_prev': np.var(diff11_list),
                'training_post': np.var(diff12_list),
            }, writer_number)

    # validate diff
    else:
        assert validate is not None
        std, prev, post, ought, real = validate.T
        diff21_list = prev - std
        diff22_list = post - std
        diff23_list = ought
        diff24_list = real
        writer.add_scalars(
            "validate_item",
            dict(
                zip(list(map(str, list(range(1,
                                             len(diff21_list) + 1)))),
                    diff21_list)), writer_number)
        writer.add_scalars(
            "difference", {
                'training_prev': np.mean(np.abs(diff11_list)),
                'training_post': np.mean(np.abs(diff12_list)),
                'training_plan': np.mean(np.abs(diff13_list)),
                'training_exec': np.mean(np.abs(diff14_list)),
                'validate_prev': np.mean(np.abs(diff21_list)),
                'validate_post': np.mean(np.abs(diff22_list)),
                'validate_plan': np.mean(np.abs(diff23_list)),
                'validate_exec': np.mean(np.abs(diff24_list)),
            }, writer_number)
        writer.add_scalars(
            "variance", {
                'training_prev': np.var(diff11_list),
                'training_post': np.var(diff12_list),
                'validate_prev': np.var(diff21_list),
                'validate_post': np.var(diff22_list),
            }, writer_number)

    # training point
    if mode >= 2:
        assert points is not None

        training_points = points[0]
        training_points = training_points.T
        training_right_drop = training_points[0]
        training_error_drop = training_points[1]
        training_right_keep = training_points[2]
        training_error_keep = training_points[3]
        training_drop_point = training_right_drop + training_error_drop
        training_all_points = training_right_drop + training_error_drop + training_right_keep + training_error_keep

        training_right_keep = training_right_keep / training_all_points
        training_error_keep = training_error_keep / training_all_points
        training_error_drop = training_error_drop / training_all_points
        training_right_drop = training_right_drop / training_all_points
        training_drop_point = training_drop_point / training_all_points
        writer.add_scalars("training_average",
                           {
                               "training_right_keep": np.average(training_right_keep),
                               "training_error_keep": np.average(training_error_keep),
                               "training_error_drop": np.average(training_error_drop),
                               "training_right_drop": np.average(training_right_drop),
                               "training_drop_point": np.average(training_drop_point),
                           }, writer_number)
        writer.add_scalars(
            "training_right_keep",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(training_right_keep) + 1)))),
                    training_right_keep)), writer_number)
        writer.add_scalars(
            "training_error_keep",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(training_error_keep) + 1)))),
                    training_error_keep)), writer_number)
        writer.add_scalars(
            "training_error_drop",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(training_error_drop) + 1)))),
                    training_error_drop)), writer_number)
        writer.add_scalars(
            "training_right_drop",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(training_right_drop) + 1)))),
                    training_right_drop)), writer_number)
        writer.add_scalars(
            "training_drop_point",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(training_drop_point) + 1)))),
                    training_drop_point)), writer_number)

    # validate point
    if mode == 3:  # do validate points
        assert points is not None and len(points) == 2

        validate_points = points[1]
        validate_points = validate_points.T
        validate_right_drop = validate_points[0]
        validate_error_drop = validate_points[1]
        validate_right_keep = validate_points[2]
        validate_error_keep = validate_points[3]
        validate_drop_point = validate_right_drop + validate_error_drop
        validate_all_points = validate_right_drop + validate_error_drop + validate_right_keep + validate_error_keep

        validate_right_keep = validate_right_keep / validate_all_points
        validate_error_keep = validate_error_keep / validate_all_points
        validate_error_drop = validate_error_drop / validate_all_points
        validate_right_drop = validate_right_drop / validate_all_points
        validate_drop_point = validate_drop_point / validate_all_points
        writer.add_scalars("validate_average",
                           {
                               "validate_right_keep": np.average(validate_right_keep),
                               "validate_error_keep": np.average(validate_error_keep),
                               "validate_error_drop": np.average(validate_error_drop),
                               "validate_right_drop": np.average(validate_right_drop),
                               "validate_drop_point": np.average(validate_drop_point),
                           }, writer_number)
        writer.add_scalars(
            "validate_right_keep",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(validate_right_keep) + 1)))),
                    validate_right_keep)), writer_number)
        writer.add_scalars(
            "validate_error_keep",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(validate_error_keep) + 1)))),
                    validate_error_keep)), writer_number)
        writer.add_scalars(
            "validate_error_drop",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(validate_error_drop) + 1)))),
                    validate_error_drop)), writer_number)
        writer.add_scalars(
            "validate_right_drop",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(validate_right_drop) + 1)))),
                    validate_right_drop)), writer_number)
        writer.add_scalars(
            "validate_drop_point",
            dict(
                zip(
                    list(map(str, list(range(1,
                                             len(validate_drop_point) + 1)))),
                    validate_drop_point)), writer_number)

def training(training_set: DatasetNewNet,
             validate_set: Union[DatasetNewNet, None] = None,
             learning_rate_start: float = LEARNING_RATE_START,
             learning_rate_max: float = LEARNING_RATE_MAX,
             learning_rate_end: float = LEARNING_RATE_END,
             epochs: int = EPOCHS,
             batch_size: int = BATCH_SIZE,
             epoch_total: int = 0,
             checkpoint: str = "",
             lossfunc=lf.Lossfunc_CrossEntropy_Weighted):

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    exc_ment, optimizer, start_epoch = load_net(checkpoint, device)

    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer=optimizer,
        max_lr=learning_rate_max,
        total_steps=epochs,
        epochs=epochs,
        steps_per_epoch=1,
        pct_start=WARMUP,
        anneal_strategy='cos',
        div_factor=learning_rate_max / learning_rate_start,
        final_div_factor=learning_rate_start / learning_rate_end,
        last_epoch=start_epoch - 1)  # type: ignore

    training_dataloader = DataLoader(dataset=training_set,
                                     batch_size=batch_size,
                                     shuffle=True,
                                     collate_fn=detection_collate,
                                     pin_memory=False,
                                     drop_last=True)
    if validate_set is not None:
        validate_dataloader = DataLoader(dataset=validate_set,
                                         batch_size=1,
                                         shuffle=True,
                                         collate_fn=detection_collate,
                                         pin_memory=False,
                                         drop_last=True)

    else:
        validate_dataloader = None

    prev_time = time.time()

    for epoch in range(start_epoch + 1, epochs + 1):
        epoch_total += 1
        for batch_index, data in enumerate(training_dataloader):
            input, coeff, masks, label, gt_energies, original_energies = [
                item.type(torch.float64).to(device) for item in data
            ]
            training_points = None
            validate_points = None
            training_result = None
            validate_result = None

            # label is the label of if keep or not of each point, containing two labels for exc and grad
            # gt_energies is the correct energy of each react from gt dataset
            # exc_energies is the exc energy of each react

            output = exc_ment(input)  # two labels for exc and grad
            '''
            point output:
            right_drop, error_drop, right_keep, error_keep
            right_drop means should drop and actually drop
            error_drop means should keep and actually drop
            right_keep means should drop and actually keep
            error_keep means should keep and actually keep

            energy output:
            exc, prev, post, ought, real
            exc is original exc energy of each reaction
            exc = 16th feature line
            prev is e_tot before action
            prev = original_energies
            post is e_tot after action
            post = prev - real
            ought is change in plan
            ought = exc_energies * label * coeff * mask then sum by dim=0 and change unit to UNIT
            real is change in act
            real = exc_energies * output * coeff * mask then sum by dim=0 and change unit to UNIT
            '''
            loss_exc = lossfunc(prediction=output[:, 0], label=label[:, 0].float())
            loss_vxc = lossfunc(prediction=output[:, 2], label=label[:, 1].float())
            loss = loss_exc + loss_vxc

            label_exc = torch.where(output[:, 0] > output[:, 1], True, False)
            label_vxc = torch.where(output[:, 2] > output[:, 3], True, False)
            label_ought = label[:, 0].type(torch.bool) & label[:, 1].type(torch.bool)
            label_real = label_exc & label_vxc
            label_ought = label_ought.unsqueeze(1)
            label_real = label_real.unsqueeze(1)

            masks = masks.type(torch.bool)
            right_drop = torch.count_nonzero((label_ought * masks) & (label_real * masks), dim=0).unsqueeze(1)
            error_drop = torch.count_nonzero((~label_ought * masks) & (label_real * masks), dim=0).unsqueeze(1)
            right_keep = torch.count_nonzero((label_ought * masks) & (~label_real * masks), dim=0).unsqueeze(1)
            error_keep = torch.count_nonzero((~label_ought * masks) & (~label_real * masks), dim=0).unsqueeze(1)

            exc = input[:, 15:16]
            ought = torch.sum(exc * label_ought * coeff * masks, dim=0) * RATIO[UNIT.lower()] / RATIO['kcal/mol']
            real = torch.sum(exc * label_real * coeff * masks, dim=0) * RATIO[UNIT.lower()] / RATIO['kcal/mol']
            exc = torch.sum(exc * coeff * masks, dim=0) * RATIO[UNIT.lower()] / RATIO['kcal/mol']
            ought = ought.unsqueeze(1)
            real = real.unsqueeze(1)
            exc = exc.unsqueeze(1)

            prev = original_energies * RATIO[UNIT.lower()] / RATIO['kcal/mol']
            post = prev - real

            training_result = torch.concat([exc, prev, post, ought, real], dim=1)
            training_points = torch.concat([right_drop, error_drop, right_keep, error_keep], dim=1)
            loss_tuple = torch.tensor([loss, loss_exc, loss_vxc]).unsqueeze(0).to(device)

            if validate_set is not None:
                assert validate_dataloader is not None
                # exc_ment_v = copy.deepcopy(exc_ment)
                loss_tuple_v = torch.tensor([0., 0., 0.]).to(device)
                count = 0
                validate_result = torch.tensor([[], [], [], [], []]).to(device).T
                validate_points = torch.tensor([[], [], [], []]).to(device).T

                for batch_index, data in enumerate(validate_dataloader):
                    input_v, coeff_v, masks_v, label_v, gt_energies_v, original_energies_v = [
                        item.type(torch.float64).to(device) for item in data
                    ]
                    with torch.no_grad():
                        output_v = exc_ment(input_v)
                    loss_exc_v = lossfunc(prediction=output_v[:, 0], label=label_v[:, 0].float())
                    loss_vxc_v = lossfunc(prediction=output_v[:, 2], label=label_v[:, 1].float())

                    length = len(output_v[:, 0])
                    loss_tuple_v[1] = loss_tuple_v[1] * count + loss_exc_v * length
                    loss_tuple_v[2] = loss_tuple_v[2] * count + loss_vxc_v * length
                    loss_tuple_v[0] = loss_tuple_v[1] + loss_tuple_v[2]

                    loss_tuple_v = loss_tuple_v * count + (loss_exc_v + loss_vxc_v) * length
                    count = count + length
                    loss_tuple_v = loss_tuple_v / count

                    label_exc_v = torch.where(output_v[:, 0] > output_v[:, 1], True, False)
                    label_vxc_v = torch.where(output_v[:, 2] > output_v[:, 3], True, False)
                    label_ought_v = label_v[:, 0].type(torch.bool) & label_v[:, 1].type(torch.bool)
                    label_real_v = label_exc_v & label_vxc_v
                    label_ought_v = label_ought_v.unsqueeze(1)
                    label_real_v = label_real_v.unsqueeze(1)

                    masks_v = masks_v.type(torch.bool)
                    right_drop_v = torch.count_nonzero((label_ought_v * masks_v) & (label_real_v * masks_v), dim=0).unsqueeze(1)
                    error_drop_v = torch.count_nonzero((~label_ought_v * masks_v) &
                                                       (label_real_v * masks_v), dim=0).unsqueeze(1)
                    right_keep_v = torch.count_nonzero((label_ought_v * masks_v) &
                                                       (~label_real_v * masks_v), dim=0).unsqueeze(1)
                    error_keep_v = torch.count_nonzero((~label_ought_v * masks_v) &
                                                       (~label_real_v * masks_v), dim=0).unsqueeze(1)

                    exc_v = input_v[:, 15:16]
                    ought_v = torch.sum(exc_v * label_ought_v * coeff_v * masks_v, dim=0) * \
                        RATIO[UNIT.lower()] / RATIO['kcal/mol']
                    real_v = torch.sum(exc_v * label_real_v * coeff_v * masks_v, dim=0) * \
                        RATIO[UNIT.lower()] / RATIO['kcal/mol']
                    exc_v = torch.sum(exc_v * coeff_v * masks_v, dim=0) * RATIO[UNIT.lower()] / RATIO['kcal/mol']
                    ought_v = ought_v.unsqueeze(1)
                    real_v = real_v.unsqueeze(1)
                    exc_v = exc_v.unsqueeze(1)

                    prev_v = original_energies_v * RATIO[UNIT.lower()] / RATIO['kcal/mol']
                    post_v = prev_v - real_v

                    validate_result_a = torch.concat([exc_v, prev_v, post_v, ought_v, real_v], dim=1)
                    validate_points_a = torch.concat([right_drop_v, error_drop_v, right_keep_v, error_keep_v], dim=1)
                    validate_result = torch.concat([validate_result, validate_result_a], dim=0)
                    validate_points = torch.concat([validate_points, validate_points_a], dim=0)
                    del input_v, coeff_v, masks_v, label_v, gt_energies_v, original_energies_v, output_v
                    torch.cuda.empty_cache()
                loss_tuple = torch.stack([loss_tuple[0], loss_tuple_v], dim=0)
                del loss_tuple_v

            lr = scheduler.get_last_lr()[-1]
            post_time = time.time()
            time_cost = post_time - prev_time
            prev_time = post_time

            writer_number = epoch * math.floor(
                max(len(training_set.data) / batch_size, 1)) + batch_index
            if training_points is None:
                points = None
            elif validate_points is None:
                points = [training_points,]
            else:
                points = [training_points, validate_points]

            writeout(writer_number=writer_number,
                     loss_tuple=loss_tuple,
                     lr=lr,
                     training=training_result,
                     validate=validate_result,
                     points=points)

            printout(epoch=epoch,
                     timecost=time_cost,
                     loss=loss_tuple,
                     lr=lr,
                     training=training_result,
                     validate=validate_result,
                     points=points)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        scheduler.step()

        if (epoch_total) % 20 == 0:
            checkpoint = {
                'net': exc_ment.state_dict(),
                'optimizer': optimizer.state_dict(),
                'epoch': epoch,
            }  # type: ignore
            save_net(
                work_dir=workdir,
                name=epoch_total,  # type: ignore
                exc_ment=exc_ment,  # type: ignore
                checkpoint=checkpoint)
    torch.save(exc_ment, workdir + '/network.net')


if __name__ == '__main__':
    try:
        torch.set_default_dtype(torch.float64)

        root = f"/opt/data/private/workspace6/Training_A_DM21/Run2"
        code = 300
        num = "{:04}".format(code)
        workdir = f"{root}/{num[:2]}/{num[2:]}"

        while code <= 10000:
            if os.path.exists(workdir):
                code += 1
                num = "{:04}".format(code)
                workdir = f"{root}/{num[:2]}/{num[2:]}"
                continue
            break
        shutil.copytree(os.path.dirname(os.path.abspath(__file__)), os.path.join(workdir, 'code_backup'))

        training_set = DatasetNewNet(
            "/opt/data/private/workspace6/Training_A_DM21/Plan/training2.plan")
        validate_set = DatasetNewNet(
            "/opt/data/private/workspace6/Training_A_DM21/Plan/validate2.plan")
        writer = SummaryWriter(log_dir=workdir)

        DATASET = training_set.data
        if validate_set.plan != []:
            DATASET.update(validate_set.data)

        WARMUP = 0.4

        learning_rate_start = 1e-06
        learning_rate_max = 1e-04
        learning_rate_end = 1e-12
        epochs = 300
        training(training_set=training_set,
                validate_set=validate_set,
                learning_rate_start=learning_rate_start,
                learning_rate_end=learning_rate_end,
                learning_rate_max=learning_rate_max,
                epochs=epochs,
                batch_size=min(len(training_set.plan), 80),
                lossfunc=lf.Lossfunc_CrossEntropy_Weighted,
                checkpoint="/opt/data/private/workspace6/Training_A_DM21/Run2/03/01/network.net")
    except Exception as e:
        [print(item) for item in e.args]
